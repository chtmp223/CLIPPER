{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all books are in the metadata file\n",
    "metadata_path = \"../../data/books/metadata/gutenberg.csv\"\n",
    "book_path = \"../../data/books/\"\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "book_txt_files = [book.split(\".\")[0] for book in os.listdir(book_path) if \".txt\" in book]\n",
    "\n",
    "book_names = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    if type(row['book_name']) != int and type(row['book_name']) != float:   \n",
    "        book_name = re.split(r'\\W+', row['author'].lower().strip())[-1]\n",
    "        if book_name not in book_txt_files:\n",
    "            for word in re.split(r'\\W+', row['title'].lower()):\n",
    "                book_name = row['author'].lower().split()[-1] + \"_\" + word.lower()\n",
    "                if book_name in book_txt_files:\n",
    "                    break\n",
    "            if book_name not in book_txt_files:\n",
    "                print(f\"Could not find book {book_name} in local folder\")\n",
    "            book_names.append(book_name)\n",
    "        else: \n",
    "            book_names.append(book_name)\n",
    "    else:       # new subset\n",
    "        book_name = int(row['book_name'])\n",
    "        book_txt_files = [int(f) for f in book_txt_files]\n",
    "        if book_name not in book_txt_files:\n",
    "            if book_name not in book_txt_files:\n",
    "                print(f\"Could not find book {book_name} in local folder\")\n",
    "            book_names.append(book_name)\n",
    "        else: \n",
    "            book_names.append(book_name)\n",
    "\n",
    "df['book_name'] = book_names    \n",
    "df.to_csv(metadata_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove book folders without the expected number of chapters \n",
    "book_path = \"../../data/books/\" \n",
    "count = 0\n",
    "import subprocess\n",
    "for i, row in tqdm(df.iterrows()): \n",
    "    book_name = row['book_name']\n",
    "    num_chapters = row['num_chapters']\n",
    "    book_file = f\"{book_path}{book_name}.txt\"\n",
    "    \n",
    "    if not os.path.exists(f\"{book_path}{book_name}\"):\n",
    "        continue\n",
    "    chapters = [ch for ch in os.listdir(f\"{book_path}{book_name}\") if \".txt\" in ch]\n",
    "    if len(chapters) != num_chapters:\n",
    "        print(f\"Expected {num_chapters} chapters for {book_name}, but found {len(chapters)}\")\n",
    "        os.system(f\"rm -r ../../data/books/{book_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which book has not been cleaned yet? aka no corresponding folder + number of chapters != ground truth \n",
    "gtruth = pd.read_csv(\"../../data/books/metadata/gutenberg.csv\")\n",
    "all_books = os.listdir(\"../../data/books/\")\n",
    "all_books = [int(book.split(\".\")[0]) for book in all_books if book not in [\"README.md\", \".DS_Store\"] and book.endswith(\".txt\")]\n",
    "uncleaned_books = []\n",
    "for book_name in all_books:\n",
    "    if not os.path.exists(f\"../../data/books/{book_name}/\"):\n",
    "        print(book_name, \"has not been cleaned yet.\")\n",
    "        uncleaned_books.append(book_name)\n",
    "    if os.path.exists(f\"../../data/books/{book_name}/\"):\n",
    "        num_chapters = len([f for f in os.listdir(f\"../../data/books/{book_name}/\") if \".txt\" in f])\n",
    "        if num_chapters != gtruth[gtruth['book_name'] == int(book_name)]['num_chapters'].values[0]:\n",
    "            print(book_name, \"has not been cleaned yet.\")\n",
    "            uncleaned_books.append(book_name)\n",
    "print(\"{} books have not been cleaned yet.\".format(len(uncleaned_books)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chapters(book_name, chapters):\n",
    "    \"\"\"Writes each chapter to a separate file in the book's folder.\"\"\"\n",
    "    book_dir = os.path.join(\"../../data/books\", str(book_name))\n",
    "    os.makedirs(book_dir, exist_ok=True)\n",
    "    for i, chapter in enumerate(chapters, start=1):\n",
    "        chapter_path = os.path.join(book_dir, f\"{i}.txt\")\n",
    "        with open(chapter_path, \"w\") as f:\n",
    "            f.write(chapter)\n",
    "    print(f\"Successfully separated {book_name} into {len(chapters)} chapters\")\n",
    "\n",
    "def dynamic_split_strategy(text, delimiter, expected_count):\n",
    "    \"\"\"\n",
    "    Tries to split the text using the given delimiter while\n",
    "    shifting the starting offset (to drop extraneous content)\n",
    "    until the expected number of chapters is reached.\n",
    "    \"\"\"    \n",
    "    parts = [chap for chap in re.split(re.escape(delimiter), text.strip()) if chap.strip()]\n",
    "    for offset in range(4):\n",
    "        chapters = parts[offset:]\n",
    "        if len(chapters) == expected_count:\n",
    "            return chapters\n",
    "    return []  \n",
    "\n",
    "def attempt_chapter_splitting(book_name, file_texts, expected_count):\n",
    "    \"\"\"\n",
    "    Try a series of splitting strategies until one produces the expected number\n",
    "    of chapters. If successful, write out the chapters and return True.\n",
    "    \"\"\"\n",
    "    strategies = [\n",
    "        (\"split_by_5_linebreaks\", \n",
    "         lambda text: [chap.strip() for chap in text.strip().split(\"\\n\\n\\n\\n\\n\") if chap.strip()]),\n",
    "\n",
    "        (\"regex_chapter\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"(?i)^(?:CHAPTER\\s+\\d+|Chapter\\s+[A-Za-z0-9]+)\\b\", text, flags=re.MULTILINE) if chap.strip()][1:]),\n",
    "\n",
    "        (\"regex_chapter_alt\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"^.*(?:CHAPTER|Chapter)\\s[A-Za-z0-9]+.*$\", text, flags=re.MULTILINE) if chap.strip()][1:]),\n",
    "\n",
    "        (\"split_5_linebreaks_dynamic\", \n",
    "         lambda text: dynamic_split_strategy(text, \"\\n\\n\\n\\n\\n\", expected_count)),\n",
    "\n",
    "        (\"split_4_linebreaks_dynamic\", \n",
    "         lambda text: dynamic_split_strategy(text, \"\\n\\n\\n\\n\", expected_count)),\n",
    "\n",
    "        (\"regex_dash_number\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"\\n-\\s*\\d+\\s*-\", text.strip()) if chap.strip()][1:]),\n",
    "\n",
    "        (\"regex_chapter_optional\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"\\n\\s?Chapter\\s*[A-Za-z0-9]+\", text.strip()) if chap.strip()][1:]),\n",
    "\n",
    "        (\"regex_roman\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"(?m)^\\s*[IVXLCDM]+\\.?\", text.strip()) if chap.strip()][1:]),\n",
    "\n",
    "        (\"regex_all_caps\", \n",
    "         lambda text: [chap.strip() for chap in re.split(\n",
    "             r\"\\n[A-Z\\s\\.]+\\n\", text.strip()) if chap.strip()][1:])\n",
    "    ]\n",
    "    \n",
    "    for strategy_name, strategy in strategies:\n",
    "        chapters = strategy(file_texts)\n",
    "        if len(chapters) == expected_count:\n",
    "            write_chapters(book_name, chapters)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Strategy {strategy_name} found {len(chapters)} chapters for {book_name} (expected {expected_count}).\")\n",
    "    return False\n",
    "\n",
    "for book_name in uncleaned_books:\n",
    "    output_dir = os.path.join(\"../../data/output\", str(book_name))\n",
    "    if book_name not in os.listdir(os.path.dirname(output_dir)):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if \"outline.md\" in os.listdir(output_dir):\n",
    "        print(f\"Skipping {book_name} because it has already been cleaned.\")\n",
    "        continue\n",
    "    if f\"{book_name}/\" in os.listdir(\"../../data/books/\"):\n",
    "        print(f\"Skipping {book_name} because it already been cleaned.\")\n",
    "        continue\n",
    "    if not str(book_name):\n",
    "        continue\n",
    "\n",
    "    expected_count = gtruth[gtruth['book_name'] == int(book_name)].num_chapters.tolist()[0]\n",
    "    \n",
    "    book_path = os.path.join(\"../../data/books\", f\"{book_name}.txt\")\n",
    "    with open(book_path, \"r\") as f:\n",
    "        chapter_text = f.read()\n",
    "\n",
    "    start_match = re.search(r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\", chapter_text)\n",
    "    start_index = start_match.end() if start_match else 0\n",
    "    end_match = re.search(r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\", chapter_text)\n",
    "    end_index = end_match.start() if end_match else len(chapter_text)\n",
    "    file_texts = chapter_text[start_index:end_index]\n",
    "    file_texts = re.sub(r\"\\[.*?\\]\", \"\", file_texts, flags=re.DOTALL)\n",
    "    \n",
    "    if attempt_chapter_splitting(book_name, file_texts, expected_count):\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Could not split {book_name} into {expected_count} chapters.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
